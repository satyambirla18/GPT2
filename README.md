# GPT-2 From Scratch

This repository contains an end-to-end implementation of the GPT-2 transformer-based language model, developed entirely from scratch using PyTorch.

---

## üîç Overview

GPT-2 (Generative Pretrained Transformer 2) is a transformer model that generates text by predicting the next word in a sequence. In this project, I implemented:

- Tokenization (custom Byte Pair Encoding or character-level)
- Positional encoding
- Masked self-attention mechanism
- Layer normalization and residual connections
- Feedforward neural network layers
- Multi-layer transformer blocks
- Training and inference loops
- Text generation using top-k and temperature sampling
